import os
import glob
import re
import pandas as pd
import torch
from tqdm import tqdm
import csv
from bert_score import score
import argparse

class BERTScoreEvaluator:
    """
    Class for evaluating the quality of summarized documents using BERTScore.
    The XLM-RoBERTa model is a multilingual model that performs well for various languages.
    """
    def __init__(self, model_type="xlm-roberta-large", batch_size=32):
        """
        Initialize the BERTScoreEvaluator with the specified model and batch size.
        """
        self.model_type = model_type
        self.batch_size = batch_size
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {self.device}")
        print(f"BERTScore model: {self.model_type}")
        print(f"Batch size: {self.batch_size}")
    
    def __call__(self, predictions, references, lang=None):
        """
        Calculate the BERTScore for the prediction texts and reference texts.
        
        Args:
            predictions: A list of summaries generated by the model
            references: A list of reference summaries
            lang: Language code (e.g., 'en', 'th', etc.)
        
        Returns:
            A dictionary containing average precision, recall, F1 scores, and detailed scores for each sample.
        """
        # Filter out empty texts
        valid_indices = []
        valid_preds = []
        valid_refs = []
        
        for i, (pred, ref) in enumerate(zip(predictions, references)):
            if len(pred.strip()) > 0 and len(ref.strip()) > 0:
                valid_indices.append(i)
                valid_preds.append(pred)
                valid_refs.append(ref)
        
        if not valid_preds:
            print("Warning: No valid prediction/reference pairs found.")
            return {"precision": 0.0, "recall": 0.0, "f1": 0.0, "scores": []}
        
        # Calculate BERTScore
        print(f"Computing BERTScore for {len(valid_preds)} samples...")
        try:
            P, R, F1 = score(valid_preds, valid_refs, lang=lang, model_type=self.model_type, 
                             device=self.device, verbose=True, batch_size=self.batch_size)
            
            # Convert tensors to Python lists
            P_list = P.tolist()
            R_list = R.tolist()
            F1_list = F1.tolist()
            
            # Reconstruct the full list according to the original indices (filtered samples are marked as -1)
            all_P = [-1] * len(predictions)
            all_R = [-1] * len(predictions)
            all_F1 = [-1] * len(predictions)
            
            for idx, orig_idx in enumerate(valid_indices):
                all_P[orig_idx] = P_list[idx]
                all_R[orig_idx] = R_list[idx]
                all_F1[orig_idx] = F1_list[idx]
            
            # Calculate averages (using only valid samples)
            avg_P = sum(P_list) / len(P_list)
            avg_R = sum(R_list) / len(R_list)
            avg_F1 = sum(F1_list) / len(F1_list)
            
            print(f"Average BERTScore - Precision: {avg_P:.4f}, Recall: {avg_R:.4f}, F1: {avg_F1:.4f}")
            
            return {
                "precision": avg_P,
                "recall": avg_R,
                "f1": avg_F1,
                "scores": list(zip(all_P, all_R, all_F1))
            }
        except Exception as e:
            print(f"Error calculating BERTScore: {str(e)}")
            return {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "scores": [(-1, -1, -1)] * len(predictions),
                "error": str(e)
            }

def get_language_code(lang_name):
    """
    Convert a language name to the language code used by BERTScore.
    """
    language_codes = {
        "english": "en",
        "thai": "th",
        "gujarati": "gu",
        "marathi": "mr",
        "pashto": "ps",
        "burmese": "my",
        "sinhala": "si"
    }
    return language_codes.get(lang_name.lower())

def is_data_file(filename):
    """
    Determine if a filename is a data file rather than a performance measurement file.
    """
    # Exclude files with 'metrics' or 'final_metrics' in the name
    if 'metrics' in filename or 'final_metrics' in filename:
        return False
    
    # Exclude result files
    if 'lid_results' in filename or 'bertscore_results' in filename or 'bertscore_detailed' in filename:
        return False
    
    # Include summaries.txt (generated by main.py)
    if filename == 'summaries.txt':
        return True
        
    # Check for language pair pattern (e.g., english-thai, thai-english)
    lang_pair_pattern = r'(english|thai|gujarati|marathi|pashto|burmese|sinhala)-(english|thai|gujarati|marathi|pashto|burmese|sinhala)'
    if re.search(lang_pair_pattern, filename):
        return True
    
    return False

def load_text_file(file_path):
    """
    Read a text file and return each line as a list.
    Multiple encodings will be attempted.
    """
    encodings_to_try = ['utf-8', 'cp1252', 'latin1', 'iso-8859-1']
    
    for encoding in encodings_to_try:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                return [line.strip() for line in f.readlines()]
        except UnicodeDecodeError:
            continue
    
    # All encoding attempts failed
    print("Could not read file with any encoding. Trying binary mode.")
    with open(file_path, 'rb') as f:
        binary_data = f.read()
        # Force conversion of binary data to UTF-8 (replace error characters)
        text = binary_data.decode('utf-8', errors='replace')
        return [line.strip() for line in text.splitlines()]

def find_reference_csv(source_lang, target_lang, reference_dir=None):
    """
    Find the reference CSV file for the given language pair.
    Checks in the CrossSum_dataset directory if reference_dir is not specified.
    """
    data_name = f"{source_lang}-{target_lang}"
    reversed_data_name = f"{target_lang}-{source_lang}"
    
    # If reference_dir is provided, look there first
    if reference_dir and os.path.exists(reference_dir):
        ref_file = os.path.join(reference_dir, "test", f"{data_name}.csv")
        if os.path.exists(ref_file):
            return ref_file
            
        # Try reversed order
        ref_file = os.path.join(reference_dir, "test", f"{reversed_data_name}.csv")
        if os.path.exists(ref_file):
            return ref_file
    
    # Try to find the CrossSum_dataset directory
    crosssum_dirs = [
        "./CrossSum_dataset/",
        "../CrossSum_dataset/",
        "../../CrossSum_dataset/"
    ]
    
    for dir_path in crosssum_dirs:
        if os.path.exists(dir_path):
            # Try direct path
            ref_file = os.path.join(dir_path, "test", f"{data_name}.csv")
            if os.path.exists(ref_file):
                return ref_file
                
            # Try reversed order
            ref_file = os.path.join(dir_path, "test", f"{reversed_data_name}.csv")
            if os.path.exists(ref_file):
                return ref_file
    
    return None

def process_language_pairs(base_dir, language_pairs, reference_dir=None, model_type="xlm-roberta-large", batch_size=32):
    """
    Calculate BERTScore for multiple language pair files.
    
    Args:
        base_dir: Directory containing generated summary files.
        language_pairs: Mapping of language pairs and file patterns.
        reference_dir: Directory containing reference CSV files. If None,
                       the reference files are searched in the CrossSum_dataset directory.
        model_type: Model type for BERTScore calculation.
        batch_size: Batch size for BERTScore calculation.
    """
    # Initialize BERTScore evaluator
    evaluator = BERTScoreEvaluator(model_type=model_type, batch_size=batch_size)
    
    # Initialize a list for storing results
    results_data = []
    
    if reference_dir is None:
        # Try to locate the CrossSum_dataset directory
        crosssum_dirs = [
            "./CrossSum_dataset/",
            "../CrossSum_dataset/",
            "../../CrossSum_dataset/"
        ]
        
        for dir_path in crosssum_dirs:
            if os.path.exists(dir_path):
                reference_dir = dir_path
                break
                
        if reference_dir is None:
            reference_dir = "./CrossSum_dataset/"  # Default value
            print(f"Warning: Could not find CrossSum_dataset directory. Using {reference_dir} as default.")
    
    for pair_name, (source_lang, target_lang, file_pattern) in language_pairs.items():
        print(f"\n{'='*50}")
        print(f"Processing language pair: {pair_name} ({source_lang} -> {target_lang})")
        print(f"{'='*50}")
        
        # Get target language code for BERTScore
        target_lang_code = get_language_code(target_lang)
        if target_lang_code is None:
            print(f"Warning: Unknown language code for {target_lang}. Using None.")
        
        # Find generated summary files matching the pattern
        pattern_path = os.path.join(base_dir, file_pattern)
        generated_files = glob.glob(pattern_path)
        
        # Also check the outputs directory structure (from main.py)
        model_output_pattern = os.path.join(base_dir, 'outputs', '*', f"{source_lang}-{target_lang}", "summaries.txt")
        generated_files.extend(glob.glob(model_output_pattern))
        
        # Filter for data files only
        generated_files = [f for f in generated_files if is_data_file(os.path.basename(f))]
        
        if not generated_files:
            print(f"No generated summary files found matching pattern: {pattern_path}")
            print(f"Also checked: {model_output_pattern}")
            continue
        
        # Find reference CSV file
        reference_file = find_reference_csv(source_lang, target_lang, reference_dir)
        
        if not reference_file:
            print(f"Reference file not found for {source_lang}-{target_lang} in {reference_dir}")
            continue
        
        print(f"Loading reference summaries from: {reference_file}")
        try:
            # Load reference summaries
            ref_df = pd.read_csv(reference_file)
            references = ref_df['summary'].tolist()
            print(f"Loaded {len(references)} reference summaries")
            
            # Print sample reference summaries
            print("\nSample reference summaries:")
            for i, ref in enumerate(references[:2]):
                print(f"{i+1}: {ref[:100]}{'...' if len(ref) > 100 else ''}")
        except Exception as e:
            print(f"Error loading reference file: {str(e)}")
            continue
        
        # Process each generated summary file
        for gen_file in generated_files:
            file_name = os.path.basename(gen_file)
            model_dir = os.path.dirname(gen_file)
            print(f"\nProcessing generated summaries from: {gen_file}")
            
            try:
                # Load generated summaries
                predictions = load_text_file(gen_file)
                print(f"Loaded {len(predictions)} generated summaries")
                
                # Filter out empty lines
                predictions = [p for p in predictions if p.strip()]
                if len(predictions) == 0:
                    print(f"Warning: No non-empty summaries found in {file_name}")
                    continue
                
                # Print sample generated summaries
                print("\nSample generated summaries:")
                for i, pred in enumerate(predictions[:2]):
                    print(f"{i+1}: {pred[:100]}{'...' if len(pred) > 100 else ''}")
                
                # Match the number of predictions with the number of references
                if len(predictions) > len(references):
                    print(f"Warning: Generated summaries ({len(predictions)}) are more than references ({len(references)}). Truncating.")
                    predictions = predictions[:len(references)]
                elif len(predictions) < len(references):
                    print(f"Warning: Generated summaries ({len(predictions)}) are fewer than references ({len(references)}). Using available ones.")
                    references = references[:len(predictions)]
                
                # Calculate BERTScore
                bertscore_results = evaluator(predictions, references, lang=target_lang_code)
                
                # Store results
                result_row = {
                    'file': gen_file,
                    'source_language': source_lang,
                    'target_language': target_lang,
                    'bertscore_precision': bertscore_results['precision'],
                    'bertscore_recall': bertscore_results['recall'],
                    'bertscore_f1': bertscore_results['f1'],
                    'samples': len(predictions)
                }
                results_data.append(result_row)
                
                # Save detailed scores to CSV
                detailed_results_file = os.path.join(model_dir, f"{os.path.splitext(file_name)[0]}_bertscore_detailed.csv")
                with open(detailed_results_file, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.writer(f)
                    writer.writerow(['index', 'prediction', 'reference', 'precision', 'recall', 'f1'])
                    
                    for i, ((p, r, f_val), pred, ref) in enumerate(zip(bertscore_results['scores'], predictions, references)):
                        if p != -1:  # -1 indicates filtered samples
                            writer.writerow([i, pred[:100], ref[:100], p, r, f_val])
                
                print(f"Detailed results saved to: {detailed_results_file}")
                
            except Exception as e:
                print(f"Error processing file {gen_file}: {str(e)}")
                result_row = {
                    'file': gen_file,
                    'source_language': source_lang,
                    'target_language': target_lang,
                    'bertscore_precision': 0.0,
                    'bertscore_recall': 0.0,
                    'bertscore_f1': 0.0,
                    'samples': 0,
                    'error': str(e)
                }
                results_data.append(result_row)
    
    # Save all results to a comprehensive CSV
    if results_data:
        results_df = pd.DataFrame(results_data)
        output_file = os.path.join(base_dir, "bertscore_evaluation_results.csv")
        results_df.to_csv(output_file, index=False)
        print(f"\nOverall BERTScore results saved to: {output_file}")
        
        # Print summary results
        print("\n" + "="*120)
        print("Summary of BERTScore Evaluation:")
        print("="*120)
        print(f"{'File':<60} | {'Language Pair':<20} | {'Precision':<10} | {'Recall':<10} | {'F1':<10} | {'Samples':<8}")
        print("-"*120)
        
        for row in results_data:
            file_short = os.path.basename(row['file'])
            pair = f"{row['source_language']} -> {row['target_language']}"
            print(f"{file_short:<60} | {pair:<20} | {row['bertscore_precision']:.4f} | {row['bertscore_recall']:.4f} | {row['bertscore_f1']:.4f} | {row['samples']:<8}")
    
    return results_data

# Language pairs definition (source language, target language, file pattern)
language_pairs = {
    'English-Thai': ('english', 'thai', '*english-thai*.txt'),
    'English-Gujarati': ('english', 'gujarati', '*english-gujarati*.txt'),
    'English-Marathi': ('english', 'marathi', '*english-marathi*.txt'),
    'English-Pashto': ('english', 'pashto', '*english-pashto*.txt'),
    'English-Burmese': ('english', 'burmese', '*english-burmese*.txt'),
    'English-Sinhala': ('english', 'sinhala', '*english-sinhala*.txt'),
    'Thai-English': ('thai', 'english', '*thai-english*.txt'),
    'Gujarati-English': ('gujarati', 'english', '*gujarati-english*.txt'),
    'Marathi-English': ('marathi', 'english', '*marathi-english*.txt'),
    'Pashto-English': ('pashto', 'english', '*pashto-english*.txt'),
    'Burmese-English': ('burmese', 'english', '*burmese-english*.txt'),
    'Sinhala-English': ('sinhala', 'english', '*sinhala-english*.txt')
}

def main():
    parser = argparse.ArgumentParser(description="Calculate BERTScore for generated summaries")
    parser.add_argument('--generated_dir', type=str, default=".", help="Directory containing generated summary files")
    parser.add_argument('--reference_dir', type=str, default=None, help="Directory containing reference CSV files (defaults to CrossSum_low)")
    parser.add_argument('--model_type', type=str, default="xlm-roberta-large", help="Model type for BERTScore calculation")
    parser.add_argument('--batch_size', type=int, default=32, help="Batch size for BERTScore calculation")
    parser.add_argument('--language_pair', type=str, default=None, 
                        choices=list(language_pairs.keys()) + ["all"], 
                        help="Specific language pair to evaluate (default: all)")
    
    args = parser.parse_args()
    
    # Process a single language pair or all pairs
    if args.language_pair and args.language_pair != "all":
        selected_pairs = {args.language_pair: language_pairs[args.language_pair]}
    else:
        selected_pairs = language_pairs
    
    # Execute evaluation
    process_language_pairs(
        args.generated_dir, 
        selected_pairs, 
        args.reference_dir,
        model_type=args.model_type,
        batch_size=args.batch_size
    )

if __name__ == "__main__":
    main()
